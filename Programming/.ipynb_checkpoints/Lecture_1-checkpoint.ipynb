{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.5.zip (1.4 MB)\n",
      "Collecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-0.15.1-py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2020.5.14-cp38-cp38-manylinux2010_x86_64.whl (689 kB)\n",
      "\u001b[K     |████████████████████████████████| 689 kB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from nltk) (4.46.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434675 sha256=34de5df608f7bb11f8aae29ee0fb88acf38f19885996875194ba029573bd2191\n",
      "  Stored in directory: /home/sls6964xx/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, joblib, regex, nltk\n",
      "Successfully installed click-7.1.2 joblib-0.15.1 nltk-3.5 regex-2020.5.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sls6964xx/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noungood\n",
      "noungood,goodness\n",
      "noungood,goodness\n",
      "nouncommodity,trade_good,good\n",
      "adjgood\n",
      "adj (s)full,good\n",
      "adjgood\n",
      "adj (s)estimable,good,honorable,respectable\n",
      "adj (s)beneficial,good\n",
      "adj (s)good\n",
      "adj (s)good,just,upright\n",
      "adj (s)adept,expert,good,practiced,proficient,skillful,skilful\n",
      "adj (s)good\n",
      "adj (s)dear,good,near\n",
      "adj (s)dependable,good,safe,secure\n",
      "adj (s)good,right,ripe\n",
      "adj (s)good,well\n",
      "adj (s)effective,good,in_effect,in_force\n",
      "adj (s)good\n",
      "adj (s)good,serious\n",
      "adj (s)good,sound\n",
      "adj (s)good,salutary\n",
      "adj (s)good,honest\n",
      "adj (s)good,undecomposed,unspoiled,unspoilt\n",
      "adj (s)good\n",
      "advwell,good\n",
      "advthoroughly,soundly,good\n"
     ]
    }
   ],
   "source": [
    "poses = {'n': 'noun',\n",
    "    'v':'verb','s':'adj (s)','a':'adj','r':'adv'}\n",
    "for synset in wn.synsets(\"good\"):\n",
    "    print(\"{}{}\".format(poses[synset.pos()],\n",
    "                       \",\".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda = wn.synset(\"panda.n.01\")\n",
    "hyper = lambda s:s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim word vector Visualation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.18.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from gensim) (1.18.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from gensim) (1.4.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-2.0.0.tar.gz (103 kB)\n",
      "Requirement already satisfied: requests in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Collecting boto\n",
      "  Using cached boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.13.19-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 22.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.17.0,>=1.16.19\n",
      "  Downloading botocore-1.16.19-py2.py3-none-any.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/sls6964xx/anaconda3/envs/LearnTFGPU/lib/python3.8/site-packages (from botocore<1.17.0,>=1.16.19->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.0.0-py3-none-any.whl size=101341 sha256=933ca049f6492aa0547890d7db0efefd21f10a2cf9d6d6a5b012791c57744bc0\n",
      "  Stored in directory: /home/sls6964xx/.cache/pip/wheels/7b/d5/3b/2814de5ed1204a33c1d234af73eabb9eae6c1bfba4cf481518\n",
      "Successfully built smart-open\n",
      "Installing collected packages: boto, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "Successfully installed boto-2.49.0 boto3-1.13.19 botocore-1.16.19 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install gensim\n",
    "import sklearn\n",
    "import numpy as np \n",
    "#Get interactive Tools for matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.test.utils import datapath,get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is word similarity package. \n",
    "For looking at word vectors, I will use Gensim We will use it in hw1 for word vectors. Gensim isnt a deep learning package. Its for word and text similarity modeling which started with (LDA-style) topic models and grew into SVD and neural word representations. But its efficient and scalable and quite widely used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standford offers GloVe word vectors. Gensim doesn't give first class support but provides a way to convert the file of GloVe to word2vec format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Glove from https://nlp.stanford.edu/projects/glove/ and extract\n",
    "#import wget\n",
    "#wget.download()\n",
    "glove_file= \"glove.6B/glove.6B.100d.txt\"\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file,word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barack', 0.937216579914093),\n",
       " ('bush', 0.927285373210907),\n",
       " ('clinton', 0.8960003852844238),\n",
       " ('mccain', 0.8875633478164673),\n",
       " ('gore', 0.8000321388244629),\n",
       " ('hillary', 0.7933663129806519),\n",
       " ('dole', 0.7851964235305786),\n",
       " ('rodham', 0.751889705657959),\n",
       " ('romney', 0.7488929629325867),\n",
       " ('kerry', 0.7472623586654663)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('obama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coconut', 0.7097253799438477),\n",
       " ('mango', 0.7054824233055115),\n",
       " ('bananas', 0.6887733936309814),\n",
       " ('potato', 0.6629636287689209),\n",
       " ('pineapple', 0.6534532904624939),\n",
       " ('fruit', 0.6519855260848999),\n",
       " ('peanut', 0.6420576572418213),\n",
       " ('pecan', 0.6349173188209534),\n",
       " ('cashew', 0.6294420957565308),\n",
       " ('papaya', 0.6246591210365295)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('banana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keyrates', 0.7173938751220703),\n",
       " ('sungrebe', 0.7119239568710327),\n",
       " ('þórður', 0.7067720890045166),\n",
       " ('zety', 0.7056615352630615),\n",
       " ('23aou94', 0.6959497928619385),\n",
       " ('___________________________________________________________',\n",
       "  0.694915235042572),\n",
       " ('elymians', 0.6945434212684631),\n",
       " ('camarina', 0.6927202939987183),\n",
       " ('ryryryryryry', 0.6905653476715088),\n",
       " ('maurilio', 0.6865653395652771)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(negative='banana') #Not useful . dissimilar words not opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen:0.7699\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive=['woman','king'],negative=['man'])\n",
    "print(\"{}:{:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(x1,x2,y1):\n",
    "    result = model.most_similar(positive=[y1,x2],negative=[x1])\n",
    "    return result[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'austrian'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('japan','japanese','austria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'german'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('japan','japanese','germany')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sls6964xx/anaconda3/envs/LearnTF/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
